services:
  ollama_server:
    image: ollama/ollama:latest
    container_name: ollama_server
    ports:
      - "11434:11434"
    networks:
      - ollama_network
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0", "1"]  # Explicitly assign GPU IDs 0 and 1
              capabilities: [gpu]
    volumes:
      - ollama:/root/.ollama  # Persistent volume for model storage
    entrypoint: >
      sh -c "
      ollama serve &
      sleep 5 &&
      if ! ollama list | grep -q 'llava:34b'; then
        echo 'Pulling llava:34b model...' &&
        ollama pull llava:34b;
      else
        echo 'llava:34b model is already available.';
      fi &&
      tail -f /dev/null
      "

  image_to_latex_api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: image_to_latex_api
    ports:
      - "5050:5050"
    environment:
      - OLLAMA_API_HOST=http://ollama_server:11434
    depends_on:
      - ollama_server
    networks:
      - ollama_network

networks:
  ollama_network:
    driver: bridge

volumes:
  ollama:
    driver: local
